# -*- coding: utf-8 -*-
"""dogs_and_cats.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w05zgwrqtun0YOBeccxHN3v5tE5Kv76Z
"""

import os
from pathlib import Path 
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow import keras
from tensorflow.keras import layers, regularizers

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/gdrive')
# %cd /gdrive/MyDrive/dogs_and_cats

import zipfile
with zipfile.ZipFile("/gdrive/MyDrive/dogs_and_cats/dogs-vs-cats.zip","r") as zip_ref:
    zip_ref.extractall("/gdrive/MyDrive/dogs_and_cats")
with zipfile.ZipFile("/gdrive/MyDrive/dogs_and_cats/train.zip","r") as zip_ref:
    zip_ref.extractall("/gdrive/MyDrive/dogs_and_cats")
with zipfile.ZipFile("/gdrive/MyDrive/dogs_and_cats/test1.zip","r") as zip_ref:
    zip_ref.extractall("/gdrive/MyDrive/dogs_and_cats")

DATA_DIR = Path("/gdrive/MyDrive/dogs_and_cats/")
IMAGE_DIR =  DATA_DIR / "train/"
MODEL_PATH = DATA_DIR / "savel_model/model1/"
RANDOM_SEED = 42
BATCH_SIZE = 64 
VALID_SIZE = 0.2
IMAGE_SIZE = (200, 200)
N_CHANNELS = 3
N_CLASSES = 2
LEARNING_RATE = 1e-3
WEIGHT_DECAY = 0.001
N_EPOCHS = 100

train_labels = list()
for name_image in os.listdir(IMAGE_DIR):
  name = name_image.split(".")[0]
  if name == "dog" :
    train_labels.append(1)
  else : train_labels.append(0)

df = pd.DataFrame({"image_ids":os.listdir(IMAGE_DIR), "labels": train_labels})
df.labels = df.labels.replace({0:"cat", 1:"dog"})
train_df, valid_df = train_test_split(
        df, test_size=VALID_SIZE, random_state=RANDOM_SEED, stratify=df.labels,
)
train_df.reset_index(drop=True ,inplace=True)
valid_df.reset_index(drop=True ,inplace=True)

train_datagen = keras.preprocessing.image.ImageDataGenerator(
    rescale=1./255,
    shear_range=0.1,
    zoom_range=0.2,
    horizontal_flip=True,
    width_shift_range=0.1,
    height_shift_range=0.1
)

train_dataset = train_datagen.flow_from_dataframe(
    train_df, 
    IMAGE_DIR, 
    x_col='image_ids',
    y_col='labels',
    target_size=IMAGE_SIZE,
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

valid_datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255)
valid_dataset = valid_datagen.flow_from_dataframe(
    valid_df, 
    IMAGE_DIR, 
    x_col='image_ids',
    y_col='labels',
    target_size=IMAGE_SIZE,
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

model = keras.Sequential([
    keras.Input(shape=(*IMAGE_SIZE, N_CHANNELS)),
    layers.Conv2D(32, 3, activation="relu", padding="same"),
    layers.BatchNormalization(),
    layers.MaxPool2D(),
    layers.Dropout(0.25),

    layers.Conv2D(64, 3, activation="relu", padding="same"),
    layers.BatchNormalization(),
    layers.MaxPool2D(),
    layers.Dropout(0.25),

    layers.Conv2D(128, 3, activation="relu", padding="same"),
    layers.BatchNormalization(),
    layers.MaxPool2D(),
    layers.Dropout(0.25),

    layers.Flatten(),
    layers.Dropout(0.5),
    layers.Dense(512, activation = "relu"),
    layers.BatchNormalization(),
    layers.Dropout(0.5),
    layers.Dense(2, activation = "softmax"),

])

model.load_weights(MODEL_PATH)

model.compile(
    optimizer=keras.optimizers.Adam(LEARNING_RATE),
    loss=keras.losses.CategoricalCrossentropy(),
    metrics=["accuracy"],
)

print(model.summary())

callbacks = [
    keras.callbacks.ModelCheckpoint(MODEL_PATH, save_weights_only=True, verbose = 1),
    keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),
    keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=1),
]

model.fit(train_dataset, validation_data=valid_dataset, callbacks=callbacks, epochs=N_EPOCHS)

